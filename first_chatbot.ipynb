{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/boachiejude/ChatBot/blob/main/first_chatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dERd7BslpREZ"
      },
      "source": [
        "**Importing the required libraries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "cgctXFn3omMM"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk\n",
        "import string\n",
        "import random\n",
        "import matplotlib as mpl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "O2oHAatBoqjN",
        "outputId": "287e0145-9b25-4ed8-bf3f-a9275d14f04b"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-1f448514d747>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/chatbot.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'ignore'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mraw_doc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mraw_doc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mraw_doc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# converting the document to lowercase\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'punkt'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# using the punkt tokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/chatbot.txt'"
          ]
        }
      ],
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "f = open('/content/chatbot.txt', 'r', errors = 'ignore')\n",
        "raw_doc = f.read()\n",
        "raw_doc = raw_doc.lower() # converting the document to lowercase\n",
        "nltk.download('punkt') # using the punkt tokenizer\n",
        "nltk.download('wordnet') # using the WordNet dictionary\n",
        "sent_tokens = nltk.sent_tokenize(raw_doc) # converts doc  to list of sentences\n",
        "word_tokens = nltk.word_tokenize(raw_doc) # converts doc to list of words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZRo4RxxX4wTQ"
      },
      "source": [
        "**Example of sentence tokens:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CsSMRFsy3T3g",
        "outputId": "d3e75ad1-e08a-4989-a7a3-c267e47c5598"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['data science\\nfrom wikipedia, the free encyclopedia\\njump to navigationjump to search\\nnot to be confused with information science.',\n",
              " 'the existence of comet neowise (here depicted as a series of red dots) was discovered by analyzing astronomical survey data acquired by a space telescope, the wide-field infrared survey explorer.']"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sent_tokens[:2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NxZ5IrTc3Wtf",
        "outputId": "7a493c03-4bb8-4ff6-9108-cc7d2e5cfc66"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['data', 'science', 'from', 'wikipedia', ',']"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "word_tokens[:5]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_aHICiyz5tgi"
      },
      "source": [
        "**Text Preprocessing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ylflYpBN47mb"
      },
      "outputs": [],
      "source": [
        "lemmer = nltk.stem.WordNetLemmatizer()\n",
        "#WordNet is a semantically-oriented dictionary of English included in NLTK.\n",
        "def LemTokens(tokens):\n",
        "  return [lemmer.lemmatize(token) for token in tokens]\n",
        "\n",
        "remove_punct_dict = dict((ord(punct), None) for punct in string.punctuation)\n",
        "\n",
        "def LemNormalize(text):\n",
        "  return LemTokens(nltk.word_tokenize(text.lower().translate(remove_punct_dict)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P-gxwn6A6ftg"
      },
      "outputs": [],
      "source": [
        "greet_inputs = ('hello', 'hi', 'greetings', 'sup', 'what\\'s up', 'hey', )\n",
        "greet_responses = ['hi', 'hey', 'nods', 'hi there', 'hello', 'I\\'m glad you\\'re talking to me']\n",
        "\n",
        "def greet(sentence):\n",
        "  for word in sentence.split():\n",
        "    if word.lower() in greet_inputs:\n",
        "      return random.choice(greet_responses)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AzcWvzrx9v_8"
      },
      "source": [
        "**Response Generation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hCms3iE99PtD"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9-k3RYy4-lIm"
      },
      "outputs": [],
      "source": [
        "def response(user_response):\n",
        "  robo1_response = ''\n",
        "  TfidVec = TfidfVectorizer(tokenizer=LemNormalize, stop_words='english')\n",
        "  tfidf = TfidVec.fit_transform(sent_tokens)\n",
        "  vals = cosine_similarity(tfidf[-1], tfidf)\n",
        "  idx = vals.argsort()[0][-2]\n",
        "  flat = vals.flatten()\n",
        "  flat.sort()\n",
        "  req_tfidf = flat[-2]\n",
        "\n",
        "  if (req_tfidf==0):\n",
        "    robo1_response = robo1_response+\"Sorry I didnt get that\"\n",
        "    return robo1_response\n",
        "  else:\n",
        "    robo1_response = robo1_response+sent_tokens[idx]\n",
        "    return robo1_response"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "30LTxWAIDdIt"
      },
      "source": [
        "**Defining conversation start/end protocols**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "UI5xgWrYC6yU",
        "outputId": "804101cc-f691-40f4-b3f5-f1fffa961d8e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BOT: My name is Atta. Let's have a conversation! Also, if you want to exit any time, just type Bye!\n",
            "BOT: I'm glad you're talking to me\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
            "  % sorted(inconsistent)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BOT: Sorry I didnt get that\n",
            "BOT: nods\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
            "  % sorted(inconsistent)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BOT: okay\n"
          ]
        }
      ],
      "source": [
        "flag = True\n",
        "print(\"BOT: My name is Atta. Let's have a conversation! Also, if you want to exit any time, just type Bye!\")\n",
        "while(flag==True):\n",
        "  user_response = input().lower()\n",
        "  if(user_response!='bye'):\n",
        "    if(user_response == 'thanks' or user_response=='thank you'):\n",
        "      flag = False\n",
        "      print('BOT: You\\'re welcome')\n",
        "    else:\n",
        "      if(greet(user_response) != None):\n",
        "        print(f\"BOT: {greet(user_response)}\")\n",
        "      else:\n",
        "        sent_tokens.append(user_response)\n",
        "        word_tokens = word_tokens + nltk.word_tokenize(user_response)\n",
        "        final_words = list(set(word_tokens))\n",
        "        print(\"BOT: \", end= \"\")\n",
        "        print(response(user_response))\n",
        "        sent_tokens.remove(user_response)\n",
        "  else:\n",
        "    flag = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1FuZn-UzEY6i"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "first_chatbot.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOuUMUr4aZxbfkvrjfnCp2w",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}